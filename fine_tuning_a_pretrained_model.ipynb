{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNEpUtjZecxlcmtzQtDrDV5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bieguni101/hugging-face-nlp-cource/blob/main/fine_tuning_a_pretrained_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96-FpVOXIqfx"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载数据集\n",
        "from datasets import load_dataset\n",
        "\n",
        "# glue dataset: https://huggingface.co/datasets/glue\n",
        "# mrpc: 是一个从在线新闻资源中自动提取的句子语料库，带有人工注释以判断句子对中的句子是否在语义上等价\n",
        "# load 得到一个包含 train 训练集、validation 验证集、test 测试集的 DatasetDict\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "\n",
        "# raw_datasets:\n",
        "# DatasetDict({\n",
        "#     train: Dataset({\n",
        "#         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
        "#         num_rows: 3668\n",
        "#     })\n",
        "#     validation: Dataset({\n",
        "#         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
        "#         num_rows: 408\n",
        "#     })\n",
        "#     test: Dataset({\n",
        "#         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
        "#         num_rows: 1725\n",
        "#     })\n",
        "# })\n",
        "\n",
        "# 按照索引可以得到具体的数据（句子对）\n",
        "# raw_train_dataset = raw_datasets[\"train\"]\n",
        "# raw_train_dataset[15]\n",
        "# {'sentence1': 'Rudder was most recently senior vice president for the Developer & Platform Evangelism Business .',\n",
        "#  'sentence2': 'Senior Vice President Eric Rudder , formerly head of the Developer and Platform Evangelism unit , will lead the new entity .',\n",
        "#  'label': 0,\n",
        "#  'idx': 16}\n",
        "\n",
        "# 按照特征可以得到该特征下的所有数据集\n",
        "# raw_datasets[\"train\"][\"sentence1\"]\n"
      ],
      "metadata": {
        "id": "4K4OiAKCLyw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 预处理数据集\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "\n",
        "# 将自然语言 token 化\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "# map 方法对数据集中的每个元素（每一类数据集）执行同一个函数\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "\n",
        "# 动态填充: 按照序列中的最大字符填充\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "t-YPJwUaORvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 开始训练\n",
        "from transformers import TrainingArguments, AutoModelForSequenceClassification, Trainer\n",
        "\n",
        "# 包括用于训练和评估的所有超参数\n",
        "training_args = TrainingArguments(\"test-trainer\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "sVUmHYcsRNMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 对训练后的模型评估\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "# predictions 包括 predictions、label_ids、metrics 三个字段\n",
        "# predictions 是一个二维数组集\n",
        "\n",
        "preds = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "# 这里可以得到模型在验证集上的准确率和 F1 分数\n",
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "metric.compute(predictions=preds, references=predictions.label_ids)"
      ],
      "metadata": {
        "id": "Hfd-IEAf4ZII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 改造数据集\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "IqFW2FEF-HBe"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 定义 loader 加载训练后的数据集\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "BRMFiGo4-M2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "# 将数据集分批传给模型\n",
        "for batch in train_dataloader:\n",
        "    break\n",
        "outputs = model(**batch)\n",
        "print(outputs.loss, outputs.logits.shape)"
      ],
      "metadata": {
        "id": "ikRuOdie-Rwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 优化器和学习率调度器\n",
        "import torch\n",
        "from transformers import get_scheduler\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# 使用从最大值（5e-5）到 0 的线性衰减作为学习率调度器\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "# print(num_training_steps)\n",
        "\n",
        "# 使用 GPU 训练，在 CPU 上训练可能需要几个小时，在 GPU 上只需要几分钟\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "# device"
      ],
      "metadata": {
        "id": "HlPnTqTp-c79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ],
      "metadata": {
        "id": "7LIlmSrwSHuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "model.eval()\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ],
      "metadata": {
        "id": "_XkRmMK0_KoE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}